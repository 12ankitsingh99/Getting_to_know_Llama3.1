{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbf47cc-7149-4cc4-b776-d5384f32d9f1",
   "metadata": {},
   "source": [
    "# RAG with Meta Llama 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90469a36-9b0e-4f43-adf9-026abaa86bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "# get a token: https://huggingface.co/docs/api-inference/quicktour#get-your-api-token\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "inference_api_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00479a1-0972-4f8e-90f3-6c87333a30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"inference_api_key\"] = inference_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79bbc8c3-695f-4f7c-8b9f-659e6401d0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to C:\\Users\\siddhanna.janai\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "repo_id =\"meta-llama/Llama-3.2-3B-Instruct\" #\"meta-llama/Llama-3.2-1B-Instruct\" #\"meta-llama/Llama-3.2-1B\"  # mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.3,\n",
    "    huggingfacehub_api_token=inference_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4cde7e5-c7b6-4c77-a96a-dbfeff1e7f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Albert Einstein (1879-1955) was a renowned German-born physicist who revolutionized our understanding of space, time, and gravity. He is widely regarded as one of the most influential scientists of the 20th century.\\nEinstein's contributions to science are numerous and groundbreaking. Some of his most notable achievements include:\\n1. **Theory of Relativity**: Einstein's theory of relativity, which includes both special relativity and general relativity, fundamentally changed our understanding of space and time.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Who is Einstein?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb81a5c-2934-4105-8742-ba1021647a74",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0232e5d0-9406-4440-b7a2-cb3b54d8d9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# Step 1: Load the document from a web url\n",
    "loader = WebBaseLoader([\"https://huggingface.co/blog/llama31\"])\n",
    "documents = loader.load()\n",
    "\n",
    "# Step 2: Split the document into chunks with a specified chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8862b31e-13c0-40b6-b9f8-fcaa5736f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "embedding_function = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=inference_api_key, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b437393f-7381-4680-8ae0-c59023e777ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Store the document into a vector store with a specific embedding model\n",
    "vectorstore = FAISS.from_documents(all_splits, embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea393b47-aec6-433a-8314-76631b535e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "#4 Query against your own data\n",
    "chain = ConversationalRetrievalChain.from_llm(llm,\n",
    "                                              vectorstore.as_retriever(),\n",
    "                                              return_source_documents=True)\n",
    "\n",
    "\n",
    "result = chain.invoke({\"question\": \"What’s new with Llama 3?\", \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "decc27a0-0ea4-4603-a606-dbe7f9331cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What’s new with Llama 3?',\n",
       " 'chat_history': [],\n",
       " 'answer': ' Llama 3 is the latest iteration in the Llama Guard family, fine-tuned on Llama 3.1 8B. It is built for production use cases, with a 128k context length and multilingual capabilities. Llama 3 can classify LLM inputs (prompts) and responses to detect content that would be considered unsafe in a risk taxonomy.\\n\\nUnhelpful Answer: Llama 3 is the latest iteration in the Llama Guard family. It is built',\n",
       " 'source_documents': [Document(metadata={'source': 'https://huggingface.co/blog/llama31', 'title': 'Llama 3.1 - 405B, 70B & 8B with multilinguality and long context', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='Llama Guard 3 is the latest iteration in the Llama Guard family, fine-tuned on Llama 3.1 8B. It is built for production use cases, with a 128k context length and multilingual capabilities. Llama Guard 3 can classify LLM inputs (prompts) and responses to detect content that would be considered unsafe in a risk taxonomy.'),\n",
       "  Document(metadata={'source': 'https://huggingface.co/blog/llama31', 'title': 'Llama 3.1 - 405B, 70B & 8B with multilinguality and long context', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='Llama 3.1 comes in three sizes: 8B for efficient deployment and development on consumer-size GPU, 70B for large-scale AI native applications, and 405B for synthetic data, LLM as a Judge or distillation. All three come in base and instruction-tuned variants.'),\n",
       "  Document(metadata={'source': 'https://huggingface.co/blog/llama31', 'title': 'Llama 3.1 - 405B, 70B & 8B with multilinguality and long context', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='osanseviero\\nOmar Sanseviero\\n\\n\\n\\n\\n\\nalvarobartt\\nAlvaro Bartolome\\n\\n\\n\\n\\n\\nlvwerra\\nLeandro von Werra\\n\\n\\n\\n\\n\\ndvilasuero\\nDaniel Vila\\n\\n\\n\\n\\n\\nreach-vb\\nVaibhav Srivastav\\n\\n\\n\\n\\n\\nmarcsun13\\nMarc Sun\\n\\n\\n\\n\\n\\npcuenq\\nPedro Cuenca\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTable of contents\\n\\nWhat‚Äôs new with Llama 3.1?\\n\\nHow much memory does Llama 3.1 need?\\nInference Memory Requirements\\n\\nTraining Memory Requirements\\n\\n\\nLlama 3.1 evaluation\\n\\nUsing Hugging Face Transformers\\n\\nHow to prompt Llama 3.1\\nBuilt-in Tool calling\\n\\n\\nCustom Tool calling\\n\\nDemo'),\n",
       "  Document(metadata={'source': 'https://huggingface.co/blog/llama31', 'title': 'Llama 3.1 - 405B, 70B & 8B with multilinguality and long context', 'description': 'We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.', 'language': 'No language found.'}, page_content='New in Llama 3.1 compared to Llama 3 is that the instruct models are fine-tuned on tool calling for agentic use cases. There are two built-in tools (search, mathematical reasoning with Wolfram Alpha) that can be expanded with custom JSON functions.')]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "829282af-c7ca-4a9f-89b6-74fc130e51e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama 3 is the latest iteration in the Llama Guard family, fine-tuned on Llama 3.1 8B. It is built for production use cases, with a 128k context length and multilingual capabilities. Llama 3 can classify LLM inputs (prompts) and responses to detect content that would be considered unsafe in a risk taxonomy.\n",
      "\n",
      "Unhelpful Answer: Llama 3 is the latest iteration in the Llama Guard family. It is built\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e089874-5a89-4d94-bc65-130115e82b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time your previous question and answer will be included as a chat history which will enable the ability\n",
    "# to ask follow up questions.\n",
    "query = \"What are the new models ?\"\n",
    "chat_history = [(query, result[\"answer\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a82b8ac-2659-486e-be0f-5532f1c46bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The new models available are the Instruct versions, which support conversational format with 4 roles: Question, System, User, and Model.\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07021176-53ce-42ec-8d87-2bf7b528e6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
