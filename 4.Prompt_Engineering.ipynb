{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jWOv74cezHe"
   },
   "source": [
    "Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jGkOtEihI7V"
   },
   "outputs": [],
   "source": [
    "pip install replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkKZV8HLexWo"
   },
   "source": [
    "Get replicate API token from here: https://replicate.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7W-jO7_NRln",
    "outputId": "93904f37-f93f-41f3-d936-6202de1d907d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    }
   ],
   "source": [
    "#First, set your Replicate API token as environment variables.\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "REPLICATE_API_TOKEN = getpass()\n",
    "\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8-f2Cd3hNRoV"
   },
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "def llama2_7b(prompt):\n",
    "    output = replicate.run(\n",
    "      \"meta/llama-2-7b-chat\",\n",
    "      input={\"prompt\": prompt}\n",
    "    )\n",
    "    return ''.join(output)\n",
    "\n",
    "def llama2_70b(prompt):\n",
    "    output = replicate.run(\n",
    "      \"meta/llama-2-70b-chat\",\n",
    "      input={\"prompt\": prompt}\n",
    "    )\n",
    "    return ''.join(output)\n",
    "\n",
    "def llama3_8b(prompt):\n",
    "    output = replicate.run(\n",
    "      \"meta/meta-llama-3-8b-instruct\",\n",
    "      input={\"prompt\": prompt}\n",
    "    )\n",
    "    return ''.join(output)\n",
    "\n",
    "def llama3_70b(prompt):\n",
    "    output = replicate.run(\n",
    "      \"meta/meta-llama-3-70b-instruct\",\n",
    "      input={\"prompt\": prompt}\n",
    "    )\n",
    "    return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ugb9odSRNRrV"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def md(t):\n",
    "  display(Markdown(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3i56UxHhBw-"
   },
   "source": [
    "In-Context Learning (e.g. Zero-shot, Few-shot)**\n",
    " * In-context learning - specific method of prompt engineering where demonstration of task are provided as part of prompt.\n",
    "  1. Zero-shot learning - model is performing tasks without any\n",
    "input examples.\n",
    "  2. Few or “N-Shot” Learning - model is performing and behaving based on input examples in user's prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "koVsxfnYNRul",
    "outputId": "ae66f3bd-c563-4516-8311-f99948e86dfc"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Amazing!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zero-shot example. To get positive/negative/neutral sentiment, we need to give examples in the prompt\n",
    "prompt = '''\n",
    "Classify: I saw an elephant.\n",
    "Sentiment: ?\n",
    "\n",
    "Give one word response.\n",
    "'''\n",
    "output = llama2_7b(prompt)\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "RaA9bgSSe_G_",
    "outputId": "a591bfeb-b7b8-415a-f361-4749f1bd3c8c"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Neutral"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = llama3_8b(prompt)\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "IJqX4m5gfNJ3",
    "outputId": "07dd6fd6-aa52-4112-ef26-9fcc76fa3e75"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Sure, I'd be happy to help! Here are the classifications and sentiments for the three questions you provided:\n",
       "\n",
       "Classify: I like to visit Zoo!\n",
       "Sentiment: Positive\n",
       "\n",
       "Classify: I don't like Snakes.\n",
       "Sentiment: Negative\n",
       "\n",
       "Classify: I saw an elephant.\n",
       "Sentiment: Positive"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# By giving examples to Llama, it understands the expected output format.\n",
    "\n",
    "prompt = '''\n",
    "Classify: I like to visit Zoo!\n",
    "Sentiment: Positive\n",
    "Classify: I dont like Snakes.\n",
    "Sentiment: Negative\n",
    "Classify: I saw an elephant.\n",
    "Sentiment:\n",
    "\n",
    "Give one word response.\n",
    "'''\n",
    "\n",
    "output = llama2_7b(prompt)\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "jeCKq8c5fNNX",
    "outputId": "83436809-4f84-4f3b-f002-aacd12ee52bb"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Fascinating"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = llama3_8b(prompt)\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hglDUsxgfiW-"
   },
   "source": [
    "Chain of Thought**\n",
    "\"Chain of thought\" enables complex reasoning through logical step by step thinking and generates meaningful and contextually relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "7jRD1eOIfkEK",
    "outputId": "8cb23f6e-4ec8-48d5-8138-4540c277b362"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Seven."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard prompting\n",
    "prompt = '''\n",
    "John had 5 tennis balls. he then buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does John have?\n",
    "\n",
    "Answer in one word.\n",
    "'''\n",
    "\n",
    "output = llama3_8b(prompt)\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "Zbjo7RhEf83_",
    "outputId": "ab361e75-83c9-45c9-8e09-e4c05214a8bb"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Eleven"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = llama3_70b(prompt)\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-GIFq6zf_Bt"
   },
   "source": [
    "Llama 3_70 b gives correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "63yl4Y5qgDxG",
    "outputId": "20c7ca8e-640e-4a6f-d555-22fb183fed89"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Let's break it down step by step!\n",
       "\n",
       "John initially had 5 tennis balls.\n",
       "\n",
       "He buys 2 more cans of tennis balls, and each can has 3 tennis balls, so he gets a total of:\n",
       "\n",
       "2 cans x 3 tennis balls/can = 6 tennis balls\n",
       "\n",
       "Adding these 6 new tennis balls to the 5 he already had, John now has:\n",
       "\n",
       "5 tennis balls + 6 tennis balls = 11 tennis balls\n",
       "\n",
       "So, John has a total of 11 tennis balls now!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# By default, Llama 3 models follow \"Chain-Of-Thought\" prompting\n",
    "prompt = '''\n",
    "John had 5 tennis balls. he then buys 2 more cans of tennis balls. Each can has 3 tennis balls.\n",
    "How many tennis balls does John have?\n",
    "'''\n",
    "\n",
    "output = llama3_8b(prompt)\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "JCNOsuqugD3e",
    "outputId": "ac229974-79ce-4e8c-ffb4-59f4e07938b3"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Let's break it down step by step!\n",
       "\n",
       "John initially had 5 tennis balls.\n",
       "\n",
       "He then buys 2 more cans of tennis balls, and each can has 3 tennis balls. So, he gets a total of:\n",
       "\n",
       "2 cans x 3 tennis balls/can = 6 new tennis balls\n",
       "\n",
       "Now, let's add the new tennis balls to the ones he already had:\n",
       "\n",
       "5 (initial tennis balls) + 6 (new tennis balls) = 11\n",
       "\n",
       "John now has a total of 11 tennis balls!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = llama3_70b(prompt)\n",
    "md(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KcnYBUQgfIj"
   },
   "source": [
    "Note: By default, Llama 3 models identify word problems and solves it step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qFR6zIeHgjTA",
    "outputId": "268358e0-74a9-4ac2-a0ab-26e8aa95ab35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "YES\n",
      "\n",
      "Here's the step-by-step breakdown:\n",
      "\n",
      "1. We have 15 people and 2 cars that can seat 5 people each. This means we can transport 2 x 5 = 10 people by car.\n",
      "2. We have 2 people left over after using the cars to transport 10 people. These 2 people can't be transported by car.\n",
      "3. We have 2 motorcycles that can fit 2 people each. This means we can transport 2 x 2 = 4 people by motorcycle.\n",
      "4. We already used 10 people by car, so we only need to transport the remaining 2 people. We can use the motorcycles to transport these 2 people.\n",
      "5. Since we have used all 10 people who can be transported by car, and all 2 people who can be transported by motorcycle, we have accounted for all 15 people. Therefore, we can all get to the restaurant by car or motorcycle.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "15 of us want to go to a restaurant.\n",
    "Two of them have cars\n",
    "Each car can seat 5 people.\n",
    "Two of us have motorcycles.\n",
    "Each motorcycle can fit 2 people.\n",
    "Can we all get to the restaurant by car or motorcycle?\n",
    "Think step by step.\n",
    "Provide the answer as a single yes/no answer first.\n",
    "Then explain each intermediate step.\n",
    "\"\"\"\n",
    "output = llama3_8b(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XTyq3dvgjXg",
    "outputId": "7dce430f-1813-44e5-9732-cbf0e125fe17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**Answer:** NO\n",
      "\n",
      "Here's the step-by-step explanation:\n",
      "\n",
      "1. We have 15 people who want to go to the restaurant.\n",
      "2. We have 2 cars, each of which can seat 5 people. This means we can transport a total of 2 x 5 = 10 people by car.\n",
      "3. This leaves 15 - 10 = 5 people who still need transportation.\n",
      "4. We have 2 motorcycles, each of which can fit 2 people. This means we can transport a total of 2 x 2 = 4 people by motorcycle.\n",
      "5. We still have 5 people who need transportation, but we can only transport 4 more people by motorcycle. This means we are short 1 person who cannot be transported by car or motorcycle.\n",
      "\n",
      "Therefore, we cannot transport all 15 people to the restaurant by car or motorcycle.\n"
     ]
    }
   ],
   "source": [
    "output = llama3_70b(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmKd3cubgx-E"
   },
   "source": [
    "Note: Llama 3 70b model works correctly in this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itijR6H0gyF4"
   },
   "source": [
    "Summary: Llama 2 often needs encouragement for step by step thinking to correctly reasoning. Llama 3 understands, reasons and explains better, making chain of thought unnecessary in the cases above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E27zqaGwg3aq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
